
# Whitepaper

```
Door: de 5 sterren-community

Versie: 1.0 (dd. 20 nov ‘24)
```



### 1. De uitdaging
Algoritmegebruik door overheden is niet transparant, met maatschappelijke schade als gevolg

Diagnose: Er is vanuit de samenleving een groeiende zorg rond algoritmes, en daarmee grote druk op de overheid om meer transparantie te bieden over de inzet en ontwikkeling van algoritmes. Wat precies verstaan wordt onder een “algoritme” is breed, complex en aan discussie onderhevig, maar bij de risico’s en uitdagingen gaat het zowel om de technologie als om de manier waarop het in processen wordt ingezet. De Algemene Rekenkamer concludeert echter in het rapport Aandacht voor algoritmes (2021) dat er op dit moment onvoldoende wordt toegezien op de kwaliteit en risico’s van de inzet van algoritmes door overheden. Maar om inzicht te krijgen in hoe een algoritme werkt en wat de effecten ervan zijn – om grip te krijgen op algoritmische processen – is transparantie daarover nodig. Echter opereren “AI-systemen soms op een weinig transparante, of zelfs onnavolgbare wijze”. 

Dat geldt in het bijzonder voor de publieke sector, waar algoritmes in toenemende mate worden toegepast om in te grijpen in de maatschappij en in mensenlevens. Bovendien zijn overheden vaak afhankelijk van de expertise van (commerciële) derde partijen voor het ontwikkelen van deze technologie, die uit zakelijke overwegingen niet altijd even bereid zijn om daarover transparantie en openheid te bieden. 

Tenslotte wordt er in beleid en wetgeving veel gehamerd op transparantie en openheid als principe of uitgangspunt, maar ontbreekt het vervolgens aan een uitvoerbare of eenduidige vertaling naar de praktijk: waarover moet precies transparantie of openheid geboden worden, op welke doelgroep zou dit gericht moeten worden en met welk eindresultaat, en hoe zouden deze maatregelen dan vorm moeten krijgen in de technische of organisatorische praktijk?  

Effect: Ondanks dat het belang van transparantie over de inzet van algoritmes duidelijk is, leidt de hierboven beschreven situatie binnen overheidsorganisaties – zowel bij bestuurders en management als in de uitvoering – tot handelingsverlegenheid, verwarring, aarzeling en frustratie bij het realiseren van transparantie en openheid rond algoritmes in de praktijk. Dat vormt een barrière voor de verantwoorde inzet van algoritmes door overheden, wat leidt tot maatschappelijk wantrouwen en zelfs schending van mensenrechten. 

### 2. Context
Het belang van transparantie, en waarom de uitvoering ervan lastig is

Transparantie – in de context van algoritmes – is als principe of vereiste sterk verankerd in wetgeving, beleid, richtlijnen en kaders. Deze documenten wekken de suggestie dat transparantie een soort moreel of maatschappelijk eindpunt is.  Maar het is cruciaal om transparantie niet als einddoel te zien, maar als een middel om een hoger doel te bereiken: inzicht bieden in de manier waarop overheidsorganisaties hun taken (met behulp van algoritmes) uitvoeren, daar als overheid verantwoording over af te kunnen leggen, en het mogelijk maken van publiek toezicht op de kwaliteit en rechtvaardigheid waarmee de overheid haar taken uitvoert. Transparantie is daarmee een essentiële voorwaarde voor het verantwoord inzetten van algoritmes. 

Omdat transparantie op dit moment nog te veel als abstract einddoel wordt gezien, is het uitvoeren daarvan in de praktijk zeer lastig. Want zonder een hoger doel waarvoor transparantie belangrijk is kunnen bepaalde vragen niet worden beantwoord, zoals voor wie (doelgroepen) transparantie belangrijk is, over welke zaken transparantie nodig is, en hoeveel transparantie in verschillende situaties gepast of genoeg is. Dat maakt dat er op dit moment een kloof bestaat tussen beleid en uitvoering, waardoor - ondanks de druk van bovenaf - het uitvoeren van transparantie in de praktijk erg lastig is. 

Deze situatie leidt tot een aantal ongewenste en schadelijke effecten: 
·	Handelingsverlegenheid, aarzeling en frustratie bij het naar de praktijk vertalen van wetgeving en beleid rond transparantie
·	Verwarring en onduidelijkheid over rollen en verantwoordelijkheden 
·	Een cultuur binnen overheidsorganisaties waarin risicovermijding de boventoon voert, en er te veel wordt gekeken naar technische maatregelen voor menselijke uitdagingen

Hieronder lichten we deze effecten verder toe. 

Er is sprake van handelingsverlegenheid, aarzeling en frustratie bij het naar de praktijk vertalen van wetgeving en beleid rond transparantie
Het is op dit moment om verschillende redenen voor overheden lastig om transparantie in de praktijk te realiseren, en dat heeft een aantal oorzaken. Ten eerste is er grote onduidelijkheid over wat transparantie als begrip betekent. De uitwerking van transparantie op beleidsniveau is vooral theoretisch en abstract, en biedt geen eenduidige, heldere blik op wat transparantie betekent binnen de context van algoritmes, laat staan binnen verschillende toepassingsgebieden.Ten tweede is er sprake van een kloof tussen beleid en uitvoering. Er is grote onduidelijkheid over hoe wetgeving en beleid concreet vorm moeten krijgen in de uitvoeringspraktijk (de algoritmische systemen zelf en de processen waarin ze worden toegepast). Dit is een bekende, lastig uitdaging met name voor overheden, en is niet beperkt tot transparantie in de context van algoritmes. In de context van algoritmes is dat in het bijzonder schrijnend, omdat transparantie zo’n essentiële voorwaarde is om schade aan burgers en de samenleving te voorkomen. 


Er is verwarring en onduidelijkheid over rollen, verantwoordelijkheden en werkwijzen

Er bestaat daarnaast ook veel onduidelijkheid over de belegging van rollen en verantwoordelijkheden binnen de organisatie die worden opgezadeld met de taak om transparantie uit te voeren. Mensen willen graag het goede doen, en dat geldt zeker ook voor professionals van de overheid die uiteindelijk werken aan een betere samenleving, voor de publieke waarden. Maar het is op dit moment totaal niet duidelijk wie verantwoordelijk is voor welke taken, of welke werkwijze er dient te worden gehanteerd, wat de uitvoering ervan sterk in de weg zit. 


Er heerst een cultuur binnen overheidsorganisaties waarin risicovermijding de boventoon voert, en er te veel wordt gekeken naar technische maatregelen voor menselijke uitdagingen

Er is binnen de overheid sprake van een heersende cultuur die niet bevorderlijk is voor transparantie en openheid. Dat kan in het algemeen al geconcludeerd worden uit de “toeslagenaffaire” bij de Belastingdienst en andere voorbeelden van overheidsorganisaties die de afgelopen tijd negatief in het nieuws zijn gekomen vanwege de toepassing van schadelijke risico- of fraude-voorspellende systemen,). We zien het ook in ons werk met of bij de overheid: er wordt sterk gestuurd op risicovermijding vanuit het perspectief van de overheid zelf, in plaats van vanuit het perspectief van de samenleving. In de praktijk leidt dat ertoe dat er wordt gestuurd op het drukken van kosten, het maximaliseren van efficiëntie, en het voorkomen van negatieve publiciteit en aandacht. 

Niet alleen staat dat haaks op transparant en open zijn; het creëert ook een cultuur waarin er sterk wordt gestuurd op meetbare, kwantificeerbare resultaten. Het resultaat daarvan is dat er bij de aanpak van uitdagingen of risico’s een overmatige focus ligt op de techniek, terwijl goede oplossingen voor maatschappelijke uitdagingen vaak liggen in een meer systematische, organisatorische, participatieve, sociale of menselijke aanpak die zich niet altijd eenvoudig laat kwantificeren. 

Vanuit het heersende mantra van de digitalisering, het datagedreven werken en de huidige hype rond algoritmes, heerst er binnen de overheid een zogeheten “technology push”: er moet geïnnoveerd worden om niet de boot te missen, technologische kansen moeten worden gepakt, maar daarmee wordt voorbij gegaan aan de vraag of technologie überhaupt de beste oplossing is voor een bepaald probleem, hoe die technologie het beste kan worden geïmplementeerd in de huidige bestaande processen, en of de (in)directe gevolgen voor de samenleving de keuze voor de technologie kunnen rechtvaardigen. 

Let op: de oorzaak van deze cultuur ligt niet alleen bij overheidsorganisaties en hun medewerkers. Ook de politiek heeft hier een groot aandeel in, met lastige top-down prikkels waar ambtenaren gehoor aan dienen te geven. Bovendien heerst binnen de media een cultuur van “sensatie”, controverse en negativiteit op dit onderwerp, wat de drempel voor overheden om transparanter te zijn nog hoger maakt. En dat terwijl tegelijkertijd vanuit het Ministerie van Binnenlandse Zaken en toezichthouders (AP, Agentschap Telecom) steeds meer wordt gehamerd en gestuurd op verplicht publiceren van algoritmes, uitvoeren van impact assessments en audits (ook op transparantie), en het centraal stellen van de burger. 

Door deze combinatie van factoren maken vooral commerciële maar ook publieke organisaties zich schuldig aan “transparency washing”: de suggestie wekken dat je transparant bent, maar daarbij bewust een keuze maken voor zaken die vanuit het perspectief van risicovermijding “veilig” is, en eigenlijk niet bijdragen aan het uiteindelijke doel van transparantie dat we eerder beschreven. 

### 3. Onze oplossing
Een raamwerk voor stapsgewijs realiseren van transparante algoritmes: Het “5 sterren-model”
Transparantie is dus een kernvoorwaarde voor verantwoorde inzet van algoritmes door overheden, maar daar wordt op dit moment nog niet (genoeg) aan voldaan. Aangespoord door grote ambities op het gebied van digitalisering groeit tegelijkertijd het aantal algoritmes dat door overheidsorganisaties wordt toegepast in rap tempo. Om die reden is de urgentie om de ongewenste en schadelijke effecten van het gebrek aan transparantie aan te pakken hoog. De oplossing ligt in het bieden van duidelijkheid en uitvoerbaarheid aan degenen die verantwoordelijk zijn voor het in de praktijk brengen van transparantie. Als oplossing introduceren wij daarom een stapsgewijs raamwerk voor transparantie: het 5 sterren-model.

NB: Deze visuele weergave van het model wordt in meer detail beschreven in het volgende hoofdstuk.

Het 5 sterren-model is een hulpmiddel dat overheidsorganisaties ondersteuning kan bieden met:

·	Helderheid bieden over transparantie-eisen, op een manier die gevoelig is voor de context van de toepassing.
·	Praktische uitvoerbaarheid: waar sta je als organisatie, waar begin je met het bieden van transparantie, waar ligt de prioriteit?
·	Organisaties en medewerkers in staat stellen om hun ambitie en doelen op het gebied van transparantie uit te drukken en te communiceren
·	Organisaties en medewerkers in staat stellen om verantwoording af te leggen over gemaakte keuzes op het gebied van transparantie
·	Het (publiek) betwistbaar maken van algoritmische systemen of processen waarin algoritmes worden gebruikt
·	Verschillende groepen belanghebbenden bedienen op basis van de specifieke behoeftes per doelgroep (i.e. burgers, ontwikkelaars, ambtenaren)
·	(Culturele) transitie teweegbrengen: transparantie moet “business as usual” zijn, en eventuele uitzonderingen verdienen zorgvuldige uitleg

Beheerd door een open community, en met Open State Foundation als hoeder

Het 5 sterren-model is een lopend project dat wordt ontwikkeld en beheerd door een open gemeenschap van een zo divers mogelijke groep mensen. Wat ons betreft is iedereen belanghebbende, of men zich daar bewust van is of niet. Iedereen die interesse heeft kan dus meewerken en wordt ook van harte uitgenodigd dat te doen. Dat kunnen organisatieteams of individuele experts zijn, maar vooral iedereen die om wat voor reden dan ook geïnteresseerd, betrokken of bezorgd is (want een belang hebben we allemaal). 

Open State Foundation neemt als betrokken en ervaren maatschappelijke organisatie op het gebied van transparantie en openheid bij de overheid de rol van ambassadeur of hoeder van het model op zich. Dat betekent dat zij hun expertise en netwerk zullen inzetten voor de verspreiding en de activatie ervan.


Voor de gehele publieke sector (en het liefst ook de rest van de organisaties)

Het 5 sterren-model is bedoeld voor alle organisaties, in het bijzonder publieke organisaties, die met algoritmes werken en openheid willen of moeten geven. Om toch wat richting te geven aan de impact van het model richt het zich in het bijzonder op twee groepen:
·	Bestuurders en leidinggevenden, als instrument om intern het gesprek te voeren over visie en ambitie, om bewustzijn te creëren, en om strategie of governance mee te articuleren en vorm te geven
·	Op innovatieprofessionals, als praktische handvatten om in projecten en innovatietrajecten transparantie om te kunnen zetten in concrete, uitvoerbare maatregelen 

NB: Het model is gericht op organisatieprocessen en toepassingscontext (i.e. ontwikkeling – inkoop – beheer – verantwoording), niet alleen op de specifieke technologie die wordt ingezet. 

### 4. Het 5 sterren-model: verdere specificatie en uitwerking

Algemeen uitgangspunt
Het 5 sterren-model een instrument dat gericht is op 1) het helpen van bestuurders en leidinggevenden bij het articuleren van hun ambities omtrent transparante algoritmes, en 2) het bieden van duidelijkheid over hoe transparantie in de praktijk verwezenlijkt kan worden. Er is geen  “toezichthouder” of certificerende partij die de sterren “uitreikt”; organisaties kunnen het model vrij gebruiken. Vrij gebruik betekent hier nadrukkelijk niet “vrijblijvend”: een toezegging of belofte om een bepaald ambitieniveau te verwezenlijken kan door (domein)toezichthouders en volksvertegenwoordigers wél gebruikt worden als toetsingsbasis. Het geeft ons als samenleving en als mensen ook de mogelijkheid om overheden te bevragen over het niet voldoen aan een bepaald niveau van transparantie, of andersom uiteraard: overheidsorganisaties kunnen ook vertrouwen en waardering winnen door wél aan bepaald niveau te voldoen. 

Wat er onder elke ster of niveau valt is zoals eerder gesteld continu in ontwikkeling om ruimte te maken voor voortschrijdend inzicht en nieuwe perspectieven; iets dat in deze razendsnel veranderende sector onmisbaar is. Hieronder geven we aan wat op dit moment per ster/niveau geldt:

1 ★: “Gepubliceerd”
De eerste ster wordt behaald wanneer er überhaupt informatie over het algoritme en de toepassing door de organisatie publiek beschikbaar is, bij voorkeur in een “algoritmeregister”. Dat betekent een beschrijving van de algemene kenmerken van het algoritme, waarin de volgende onderdelen minimaal worden besproken:
·	Beschrijving van het doel/de functie van het algoritme;
·	Beschrijving van de rol van het algoritme in relatie tot het proces waarin het gebruikt wordt;
·	Een risicoclassificatie van het algoritme.

Met deze informatie informeer je over het gebruik van het algoritme, waarom het wordt ingezet, welke risico’s er zijn, wat de (indirecte) impact is en welke keuze gemaakt zijn om bijvoorbeeld risico’s in te perken en waarborgen te stellen. Zodat individuen en andere stakeholders kunnen weten dat een algoritme ingezet wordt en kunnen bekijken hoe dit impact heeft. Deze informatie moet beschikbaar worden gesteld in begrijpelijke taal: B1-niveau.  
2 ★:  “Uitgelegd”
Op dit niveau wordt beschreven wat er gedaan is/wordt om te zorgen dat het algoritme doet wat het moet doen. Er wordt hiervoor een beschrijving gegeven van hoe het algoritme is getraind, geoptimaliseerd en op bias is gecontroleerd. Denk hierbij bijvoorbeeld aan de volgende onderdelen:
·	Een reflectie op normatieve keuzes en definities (bijv. de keuze voor een bepaalde eerlijkheidsdefinitie);
·	Welke optimalisatiecriteria er gehanteerd worden;
·	Welke beheersmaatregelen er toegepast zijn/gaan worden (bijv. testing/impact assessment);
·	Wie of welke partijen verantwoordelijk zijn voor de life-cycle/toezicht/verbetering;
·	Een stroomschema van de inputs en outputs gedurende het proces.

Met deze toelichting informeer je ook over de wijze waarop de werking en de effecten van het algoritme wordt gemonitord, zodat het algoritme blijft doen wat het moet. Hiermee biedt je transparantie over de gevolgen van de toepassing van het algoritme op de langere termijn, en op welke maatregelen of afspraken later geëvalueerd kan worden.

3 ★:  “Gecontroleerd”
Op dit niveau wordt niet alleen beschreven wat er gedaan is om te zorgen dat het algoritme doet wat het moet doen, maar kan dit ook aangetoond worden. Hierbij worden resultaten gedeeld die laten zien dat het algoritme  “goed” en  “eerlijk” werkt. Dit kan bijvoorbeeld aangetoond worden door het delen van:
·	Auditresultaten;
·	Monitoringsdata;
·	Resultaten van ethische toetsing (bijv. van een eventuele ethische commissie, of een IAMA-traject (Impact Assessment Mensenrechten en Algoritmes);
·	Hoe het algoritme getest en gevalideerd is, en wat daarvan de resultaten zijn;
·	Hoe ervoor gezorgd is dat de kwaliteit van de trainingsdata voldoende is, ook vanuit ethisch perspectief;
·	Andere resultaten van de “checks en balances” die onderdeel vormen van de life-cycle .

Het aantonen van een “goede”  en “eerlijke” werking, helpt bij het opbouwen van extern vertrouwen in de werking en toepassing van een algoritme. Het geeft nader zicht op de validiteit en betrouwbaarheid van de gegeven informatie.

4 ★: “Testbaar”
Op dit niveau kunnen belanghebbenden zelf het algoritme gaan testen. Mensen kunnen dan zelf inzien welke output er geleverd wordt bij welke input. Dit kan bijvoorbeeld gerealiseerd worden middels:
·	Een API waarmee resultaten opgevraagd kunnen worden;
·	Een “mock-up” systeem (wanneer volledige openheid niet geboden kan worden uit b.v. veiligheidsoverwegingen);
·	Test-data, eventueel “synthetisch” (niet de daadwerkelijk gevoelige data maar een vergelijkbare neutrale set)

Hiermee wordt het algoritme en de werking tastbaar en daarmee ook testbaar. Dat is een voorwaarde om een open gesprek te kunnen voeren over waarom het algoritme tot bepaalde uitkomsten komt, hoe deze resultaten geïnterpreteerd moeten worden, of deze uitkomsten wenselijk zijn, en (dus) of de toepassing van het algoritme in een bepaald proces verantwoord en gerechtvaardigd is. 

5 ★: “Open”
Op dit niveau is het algoritme volledig open. De code, data, beheersmaatregelen en ontwerpkeuzes zijn volledig inzichtelijk, uiteraard op een manier waarbij de privacy van de ontwikkelaars en andere betrokkenen niet wordt aangetast of voor onveilige situaties kan zorgen. De 5 sterren voor maximale openheid is bedoeld om experts of toezichthoudende partijen de mogelijkheid te geven om het algoritme volledig te doorgronden. Concreet kan dit bijvoorbeeld door middel van:
·	Het beschikbaar stellen van de broncode in combinatie met contextuele informatie, zoals een “model card”
·	Het beschikbaar stellen van de trainingsdata
·	Het volledig transparant zijn over de door de organisatie gemaakte afwegingen en effecten m.b.t. de toepassing van het algoritme

### Belangrijke overwegingen bij (toepassing van) het model
Er zullen bij het lezen van dit whitepaper mensen zijn die denken: zo’n ambitie is leuk, maar niet helemaal eerlijk, want niet elk algoritme kán vijf sterren halen. Dat is een terecht punt, wanneer bijvoorbeeld  privacy (denk aan persoonsgegevens in trainingsdata) of nationale veiligheid (het risico dat criminelen hun voordeel doen met gepubliceerde informatie over beveiligingssystemen) volledige openheid onmogelijk maken. Het hoogst haalbare ambitieniveau dient dus altijd in context bekeken te worden, en wanneer geen volledig transparantie geboden kan worden, dan moeten organisaties daar niet zonder meer direct op afgerekend te worden. Mits daar een gegronde afweging tegenover staat. En over de afwegingen die aan deze besluiten ten grondslag liggen kan transparantie geboden worden. 

Ook is het belangrijk dat het 5 sterren-model niet als een checklist of een uitputtend overzicht wordt gezien, maar als een richtinggevend hulpmiddel voor ambitieniveaus. We pogen niet uitputtend te zijn, maar willen ambitieniveaus schetsen. Het kan per organisatie verschillen wat er nodig is om een sterrenniveau te behalen, omdat het afhangt van de instrumenten die gebruikt worden, en de volwassenheid op het gebied van bijvoorbeeld ethiek en data governance. We raden je daarom aan om voor je organisatie een vertaling te maken van waar je staat, wat er aan werkzaamheden nodig is om op een bepaald ambitieniveau te komen, en waar de prioriteit ligt. Daarbij kan het 5 sterren-model helpen als leidraad om intern het gesprek te voeren, en het proces vorm te geven.

Tot slot: publieke organisaties moeten stappen zetten op het gebied van transparante algoritmes, en dat vereist tijd, ruimte en middelen. Het 5 sterren-model helpt om te concretiseren welke stappen er gezet zouden kunnen of moeten worden, maar zonder het vrijmaken van die tijd en ruimte binnen de organisatie en het bieden van handelingsruimte aan medewerkers om hiermee aan de slag te gaan zal de uitvoering spaak lopen. Dat is niet alleen vanuit moreel oogpunt problematisch; door een gebrek aan transparantie komen risico’s en problemen bij de toepassing van algoritmes pas veel later aan het licht, wanneer de schade ervan groter is en het rechtzetten ervan kostbaarder. We raden daarom aan – naast de noodzaak om bestaande systemen transparanter te maken – om in ieder geval voor nieuwe projecten openheid als randvoorwaarde (“definition-of-done”) mee te nemen. 

### 6. Tips om met het model aan de slag te gaan
Het 5 sterren-model is als instrument gericht op het articuleren van organisatorische ambities, om te helpen het gesprek te voeren en urgentie en bewustzijn te creëren. Dit zou ook het startpunt moeten zijn voor de toepassing van het model binnen de organisatie. Maar het voorziet daarnaast ook in het bieden van handvatten om deze ambities uitvoerbaar te maken, om tastbare resultaten in de praktijk mee te realiseren. De volgende stappen kunnen helpen om met het model aan de slag te gaan:  
1.	Maak op bestuurlijk niveau afspraken over een ambitieniveau dat je voor de organisatie beoogt te bereiken;
2.	Beschrijf helder wat transparantie voor jouw organisatie betekent, met aandacht voor de specifieke praktische context waar jouw organisatie mee te maken heeft, en de kernwaarden van je organisatie;
3.	Beleg de verantwoordelijkheden is voor de realisatie van 1 en 2 duidelijk binnen je organisatie, en maak daarvoor ruimte en middelen vrij;
4.	Inventariseer welke algoritmes er binnen je organisatie worden gebruikt, en waarvoor;
5.	Verzamel minimale informatie over het algoritme;
6.	Bepaal het risiconiveau en de impact (zoals ook vanuit de AI Act een verplichting wordt);
7.	Registreer en publiceer volgens de organisatierichtlijnen (stap 2);
8.	Transparantie begint al aan de voorkant, want zonder de juiste inkoopvoorwaarden wordt het bieden van transparantie op een later moment lastig. Neem dit dus al mee bij het inkopen van de technologie. 


### 7. Conclusie en call to arms!
Dit raamwerk helpt organisaties in kaart te brengen waar ze staan en waar ze moeten beginnen, waar nog werk nodig is, en in welke volgorde, zodat tijd en middelen goed besteed worden. Verlaag de drempel, vergroot het handelingsvermogen, verhoog het bewustzijn, en wees open. Door het van de gemeenschap te maken borg je deskundigheid maar ook diversiteit en inclusie. We roepen iedereen daarom op om mee te helpen bij de volgende essentiële taken:

·	Doe (denk, bouw en praat) mee!
·	Spread the word: ambassadeurs op alle lagen – in de board rooms, binnen de universiteiten, op de redacties van media, bij de kopieerapparaten, in de kroeg – zijn nodig om een beweging op gang te brengen;
·	Pilot cases om model op te “testen”: Organisaties die bereid zijn? Systemen die kansrijk zijn? 
·	Nay-sayers, say your say! We roepen kritische blikken en gesignaleerde beren op de weg actief op om ons te overtuigen van hoe het wél zou moeten;
·	...




Achtergrond/literatuur

Morley, J., Floridi, L., Kinsey, L. et al. From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices. Sci Eng Ethics 26, 2141–2168 (2020). https://doi.org/10.1007/s11948-019-00165-5

Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D. Weisz. 2021. Expanding Explainability: Towards Social Transparency in AI systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 82, 1–19. https://doi.org/10.1145/3411764.3445188

Alfrink, K., Keller, I., Kortuem, G. et al. Contestable AI by Design: Towards a Framework. Minds & Machines (2022). https://doi.org/10.1007/s11023-022-09611-z

Zalnieriute, Monika, “Transparency-Washing” in the Digital Age: A Corporate Agenda of Procedural Fetishism (2021). Critical Analysis of Law, 8(1) 2021, pp. 39-53, UNSW Law Research Paper No. 21-33, Available at SSRN: https://ssrn.com/abstract=3805492

Prem, E. From ethical AI frameworks to tools: a review of approaches. AI Ethics 3, 699–716 (2023).

K. Haresamudram, S. Larsson and F. Heintz, "Three Levels of AI Transparency," in Computer, vol. 56, no. 2, pp. 93-100, Feb. 2023, doi: 10.1109/MC.2022.3213181

Eschenbach, W.J. von. Transparency and the Black Box Problem: Why We Do Not Trust AI. Philos. Technol. 34, 1607–1622 (2021). https://doi.org/10.1007/s13347-021-00477-0






### Why

### What

### How

